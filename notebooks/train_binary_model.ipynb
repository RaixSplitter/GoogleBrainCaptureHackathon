{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from model import BendrEncoder\n",
    "from model.model import Flatten\n",
    "\n",
    "from src.data.conf.eeg_annotations import braincapture_annotations\n",
    "from net1d import Net1D\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# set seed\n",
    "import random\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Suppress logger messages from MNE-Python\n",
    "mne_logger = logging.getLogger('mne')\n",
    "mne_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "encoder = BendrEncoder()\n",
    "\n",
    "# Load the pretrained model\n",
    "encoder.load_state_dict(deepcopy(torch.load(\"encoder.pt\", map_location=device)))\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, optimizer):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total = correct = 0\n",
    "        pbar = tqdm(total=len(test_loader), desc=f\"Testing...\")\n",
    "        for batch in test_loader:\n",
    "            if len(batch[0]) < 2: continue            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "                    \n",
    "            X, y = batch\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            pbar.update(1)\n",
    "            \n",
    "        return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary dataset\n",
    "X_true = torch.load(\"X_augment_data.pt\").to(device)\n",
    "X_true = X_true[:,:,:-1] # the feature vector len is 1537 for the false it is 1536 - easy fix / hack\n",
    "# y_true = torch.load(\"y_augment_data.pt\").to(device)\n",
    "y_true = torch.ones(X_true.shape[0])\n",
    "print(X_true.shape, y_true.shape)\n",
    "\n",
    "X_false = torch.load(\"Z_data.pt\").to(device)\n",
    "X_false = X_false[torch.randperm(len(X_false))] # shuffle the tensor before discarding half for \"test data\"\n",
    "X_false = X_false[len(X_false)//2:] # discard half to help class imbalance\n",
    "y_false = torch.zeros(X_false.shape[0])\n",
    "print(X_false.shape, y_false.shape)\n",
    "\n",
    "X = torch.cat((X_true, X_false), dim=0)\n",
    "y = torch.cat((y_true, y_false), dim=0)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "X_val = torch.load(\"X_data.pt\").to(device)\n",
    "y_val = torch.load(\"y_data.pt\").to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X, y)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(train_dataset, test_size=0.2)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = DataLoader(train_loader, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "weight_decay=0.01\n",
    "n_epochs = 5\n",
    "n_splits = 5\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.BCEWithLogitsLoss(weight=torch.tensor([0.1, 0.9]))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=learning_rate, epochs=n_epochs, steps_per_epoch=len(train_loader), pct_start=0.1, last_epoch=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=n_splits)\n",
    "accuracies = []\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(train_dataset)):\n",
    "    print(f'Fold {fold}')\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_index)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_index)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, sampler=train_subsampler)\n",
    "    val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, sampler=val_subsampler)\n",
    "\n",
    "    out_features = 2 # binary\n",
    "    model = nn.Sequential(\n",
    "        encoder,\n",
    "        Flatten(),\n",
    "        nn.Linear(in_features = 3 * 512 * 4, out_features = 512 * 4, bias=True),\n",
    "        nn.Dropout(p=0.5, inplace=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(512 * 4),\n",
    "        nn.Linear(512 * 4, out_features, bias=True) \n",
    "    ).to(device)\n",
    "    model = torch.load(\"models/binary_model_fold1_epoch5.pt\")\n",
    "    model = model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = nn.BCEWithLogitsLoss(weight=torch.tensor([0.1, 0.9]))\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=0.0001, epochs=n_epochs, steps_per_epoch=len(train_loader), pct_start=0.1, last_epoch=-1\n",
    "    )\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        total = correct = 0\n",
    "        pbar = tqdm(total = len(train_loader), desc = f\"Epoch {epoch}, train\")\n",
    "\n",
    "        for vector, label in train_loader:\n",
    "            #print(vector.shape)\n",
    "            #if len(vector) < 2: continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            vector = vector.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            onehot_label = torch.torch.nn.functional.one_hot(label.to(torch.int64), 2).float()\n",
    "\n",
    "            logits = model(vector)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "            \n",
    "            loss = criterion(logits, onehot_label)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        test_accuracy = evaluate_model(model, test_loader, device, optimizer)\n",
    "        pbar.set_description(f\"Epoch {epoch}, train: {train_accuracy:.2f}%, test: {test_accuracy:.2f}%\")\n",
    "        torch.save(model.state_dict(), f\"models/binary_model_fold{fold}_epoch{epoch+1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
